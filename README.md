# ğŸ¥ IMDb Review Classifier â€” Turning Text into Insight

> Can a machine understand a movie review?  
> With the right preprocessing, vectorization, and models â€” **yes, it can.**

---

## ğŸ” Whatâ€™s This Project About?

This project takes 50,000 raw IMDb movie reviews with sentiment and builds a complete **sentiment analysis pipeline** using Python, NLP, and Machine Learning. 

Built from scratch in Python, this project showcases how a well-crafted pipeline â€” from text cleaning to vectorization to model selection â€” can produce impressive accuracy even with simple models.

---

## ğŸ’¡ Key Highlights

âœ… **Text Cleaning & Preprocessing**  
- Removed HTML tags, punctuation, special characters  
- Applied **lemmatization** using NLTK  
- Removed stopwords for cleaner signal  

âœ… **Efficient Data Processing**  
- Used **Swifter** to accelerate `.apply()` functions  
- Leveraged **regular expressions** for robust text pattern handling  

âœ… **Feature Engineering**  
- Converted text into vectors using **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)**  
- Tuned max features and n-gram ranges  

âœ… **Modeling & Evaluation**  
- Built and evaluated two strong baseline models:  
  - **Logistic Regression**  
  - **Multinomial Naive Bayes**  
- Compared using accuracy, precision, recall, and F1-score  

---

## ğŸ§  Technologies Used

- ğŸ **Python 3**  
- ğŸ›  **Pandas**, **NumPy**  
- ğŸ“š **NLTK** for NLP tasks  
- ğŸ“ **Scikit-learn** for modeling and evaluation  
- âš¡ **Swifter** for faster DataFrame operations  
- ğŸ§¹ **Regex**, `string`, and `re` modules for text cleaning

---

## ğŸ¯ Project Goal

To train supervised ML models on labeled IMDb movie reviews to predict sentiment, using end-to-end NLP techniques â€” from raw text to vectorized features to final classification â€” and demonstrate that even baseline models can achieve strong performance when the data pipeline is solid.

---

## ğŸ“Š Results

| Model                   | Accuracy |
|------------------------|----------|
| Logistic Regression     | 86.1%    |
| Multinomial Naive Bayes | 83.3%    |

ğŸ” **Observation**: Logistic Regression outperformed Naive Bayes, likely due to its ability to weigh features more flexibly in high-dimensional TF-IDF space.

---

ğŸ“˜ Learnings & Takeaways
âœ” Text data is messy, but structured preprocessing can create clarity
âœ” Simple models can go a long way with the right features
âœ” Building your own pipeline teaches far more than using prebuilt tools
âœ” Even with 50,000 reviews, speed matters â€” swifter and vectorizers save time

ğŸ™‹â€â™€ï¸ About Me
I'm a budding data scientist with a passion for NLP, modeling, and turning raw data into real-world impact.
Connect with me to talk data, code, or cinema ğŸ¬

ğŸ”— GitHub: mubashera124

ğŸ’¼ LinkedIn: https://www.linkedin.com/in/mubashera-siddiqui-59489823a/



