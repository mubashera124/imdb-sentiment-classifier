# ğŸ¥ IMDb Review Classifier â€” Turning Text into Insight

> Can a machine understand a movie review?  
> With the right preprocessing, vectorization, and models â€” **yes, it can.**

---

## ğŸ” Whatâ€™s This Project About?

This project takes 50,000 raw IMDb movie reviews and turns them into meaningful sentiment predictions â€” **positive** or **negative** â€” using the power of **Natural Language Processing (NLP)** and **Supervised Machine Learning**.

Built from scratch in Python, this project showcases how a well-crafted pipeline â€” from text cleaning to vectorization to model selection â€” can produce impressive accuracy even with simple models.

---

## ğŸ’¡ Key Highlights

âœ… **Text Cleaning & Preprocessing**  
- Removed HTML tags, punctuation, special characters  
- Applied **lemmatization** using NLTK  
- Removed stopwords for cleaner signal  

âœ… **Efficient Data Processing**  
- Used **Swifter** to accelerate `.apply()` functions  
- Leveraged **regular expressions** for robust text pattern handling  

âœ… **Feature Engineering**  
- Converted text into vectors using **TF-IDF (Term Frequencyâ€“Inverse Document Frequency)**  
- Tuned max features and n-gram ranges  

âœ… **Modeling & Evaluation**  
- Built and evaluated two strong baseline models:  
  - **Logistic Regression**  
  - **Multinomial Naive Bayes**  
- Compared using accuracy, precision, recall, and F1-score  

---

## ğŸ§  Technologies Used

- ğŸ **Python 3**  
- ğŸ›  **Pandas**, **NumPy**  
- ğŸ“š **NLTK** for NLP tasks  
- ğŸ“ **Scikit-learn** for modeling and evaluation  
- âš¡ **Swifter** for faster DataFrame operations  
- ğŸ§¹ **Regex**, `string`, and `re` modules for text cleaning

---

## ğŸ¯ Project Goal

To train supervised ML models on labeled IMDb movie reviews to predict sentiment, using end-to-end NLP techniques â€” from raw text to vectorized features to final classification â€” and demonstrate that even baseline models can achieve strong performance when the data pipeline is solid.

---

## ğŸ“Š Results

| Model                   | Accuracy |
|------------------------|----------|
| Logistic Regression     | 86.1%    |
| Multinomial Naive Bayes | 83.3%    |

ğŸ” **Observation**: Logistic Regression outperformed Naive Bayes, likely due to its ability to weigh features more flexibly in high-dimensional TF-IDF space.

---

## ğŸ“‚ File Structure

